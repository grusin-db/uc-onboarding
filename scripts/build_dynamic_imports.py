import json
import re
from datetime import datetime
import argparse
from databricks.sdk import WorkspaceClient, AccountClient
from typing import Optional

def generate_import_snippet(provider:str, id:str, to:str) -> str:
  """generates terraform's 1.5 import block for given resource

  Args:
      provider (str): name of the provider to use
      id (str): id of resource
      to (str): name of resource

  Returns:
      str: import { ... } block
  """  
  return f"""
import {{
  to       = {to}
  id       = {id}
  provider = {provider}
}}
"""

def get_json_dict(file_name:str):
  with open(file_name) as json_file:
    d = json.load(json_file)
    if not isinstance(d, dict):
      raise ValueError("not a dict")
    
    return d

def get_json_records(file_name:str):
  """returns records from json file

  Args:
      file_name (str): name of json file containing list of objects as it's contents

  Yields:
      Iterator: iterator for each record in a file
  """  
  with open(file_name) as json_file:
    for d in json.load(json_file):
      if "enabled" not in d or d['enabled']:
        yield d
        
def get_single_json_record(file_name:str) -> dict:
  """returns single entry from json file

  Args:
      file_name (str): name of json file containing list of objects as it's contents

  Raises:
      ValueError: in case file contains no, or more than 1 record exception is returned

  Returns:
      _type_: _description_
  """  
  records = list(get_json_records(file_name))
  if len(records) != 1:
    raise ValueError(f"expected one record got {len(records)} for {file_name}")
  
  return records[0]

def get_uc_catalog(wc: WorkspaceClient, name: str) -> Optional[dict]:
  """gets uc catalog defintion

  Args:
      wc (WorkspaceClient): databricks sdk WorkspaceClient
      name (str): name of the catalog to get

  Returns:
      Optional[dict]: None if catalog does not exist, otherwise dictionary with result of API call
  """  
  try:
    return wc.catalogs.get(name)
  except Exception:
    return None

def get_uc_storage_credentials(wc: WorkspaceClient, name: str) -> Optional[dict]:
  """gets uc storage credentials

  Args:
      wc (WorkspaceClient): databricks sdk WorkspaceClient
      name (str): name of the storage credential to get

  Returns:
      Optional[dict]: None if name does not exist, otherwise dictionary with result of API call
  """  
  try:
    return wc.storage_credentials.get(name)
  except Exception:
    return None
  
def get_uc_external_locations(wc: WorkspaceClient, name: str) -> Optional[dict]:
  """gets uc external location

  Args:
      wc (WorkspaceClient): databricks sdk WorkspaceClient
      name (str): name of the external location

  Returns:
      Optional[dict]: None if name does not exist, otherwise dictionary with result of API call
  """  
  try:
    return wc.external_locations.get(name)
  except Exception:
    return None
  
def get_uc_metastore(wc: WorkspaceClient, name: str):
  """gets uc metastore

  Args:
      wc (WorkspaceClient): databricks sdk WorkspaceClient
      name (str): name of the metasotre

  Returns:
      Optional[dict]: None if name does not exist, otherwise dictionary with result of API call
  """  
  try:
    for m in wc.metastores.list():
      if m.name == name:
        return m 

  except Exception:
    return None
  
if __name__ == "__main__":
  env_setup = get_json_dict(".metadata.tmp/current_environment.json")
  dbr_host = env_setup['admin_databricks_worskapce_url']
  dbr_account_id = env_setup['databricks_account_id']
  wc = WorkspaceClient(host=dbr_host)
  
  with open('dynamic-imports.tf', 'w') as out:
    out.write(f"""#
# This file was auto generated by {__file__} script.
# Any changes made to this file will be lost.
#
""")
    
    databricks_admin_provider = 'databricks.admin'
    databricks_account_provider = 'databricks.account'
    
    #catalogs
    out.write("\n#\n# catalogs\n#\n")
    for d in get_json_records(".metadata.tmp/catalogs.json"):
      if get_uc_catalog(wc, d['name']):   
        id = '"' + d['name'] + '"'
        to = f'databricks_catalog.catalog[{id}]'
        out.write(generate_import_snippet(databricks_admin_provider, id, to))
    
    #metastores
    out.write("\n#\n# metastores\n#\n")
    d = get_single_json_record(".metadata.tmp/metastores.json")
    metastore = get_uc_metastore(wc, d['name'])
    if metastore:
      id = '"' + metastore.metastore_id + '"'
      to = 'databricks_metastore.this'
      out.write(generate_import_snippet(databricks_account_provider, id, to))
    
    #storage-credentials
    out.write("\n#\n# storage-credentials\n#\n")
    for d in get_json_records(".metadata.tmp/storage-credentials.json"):
      if get_uc_storage_credentials(wc, d['name']):
        id = '"' + d['name'] + '"'
        to = f'databricks_storage_credential.storage_credential[{id}]'
        out.write(generate_import_snippet(databricks_account_provider, id, to))
      
    #storage-locations
    out.write("\n#\n# storage-locations\n#\n")
    for d in get_json_records(".metadata.tmp/storage-locations.json"):
      if get_uc_external_locations(wc, d['name']):
        id = '"' + d['name'] + '"'
        to = f'databricks_external_location.storage_location[{id}]'
        out.write(generate_import_snippet(databricks_admin_provider, id, to))
      
    #workspaces
    #not needed, api's are already indempotent!
      
    